#############################  Display  #############################

# Verbose program output
# Accepts log level: "error", "warn", "info", "debug", "trace"
verbose = "info"

# Output format
format = "detailed"

# Output display mode
mode = "emoji"

# Don't show interactive progress bar while checking links.
no_progress = false

# Path to summary output file.
output = ".config.dummy.report.md"

# Extract links instead of checking them
dump = true

# Dump inputs instead of doing link extraction / checking
dump_inputs = true

#############################  Cache  ###############################

# Enable link caching. This can be helpful to avoid checking the same links on
# multiple runs.
cache = true

# Discard all cached requests older than this duration.
max_cache_age = "2d"

# A list of status codes that will be ignored from the cache
cache_exclude_status = "500.."

#############################  Runtime  #############################

# File to read and write cookies
cookie_jar = "cookie-jar"

# Number of threads to utilize.
# Defaults to number of cores available to the system if omitted.
threads = 2

# Maximum number of allowed redirects.
max_redirects = 10

# Maximum number of allowed retries before a link is declared dead.
max_retries = 2

# Maximum number of concurrent link checks.
max_concurrency = 14

# extension applied to files without extension
default_extension = "md"

# GitHub API token
github_token = "secret"

# Resolve directories to index files
index_files = ["index.html"]

# Preprocess input files
preprocess = { command = "preprocess.sh" }

#############################  Requests  ############################

# User agent to send with each request.
user_agent = "curl/7.83. 1"

# Website timeout from connect to response finished.
timeout = 20

# Minimum wait time in seconds between retries of failed requests.
retry_wait_time = 2

# Comma-separated list of accepted status codes for valid links.
# Supported values are:
#
# accept = ["200..=204", "429"]
# accept = "200..=204, 429"
# accept = ["200", "429"]
# accept = "200, 429"
accept = ["200", "429"]

# Proceed for server connections considered insecure (invalid TLS).
insecure = false

# Only test links with the given schemes (e.g. https).
# Omit to check links with any other scheme.
# At the moment, we support http, https, file, and mailto.
scheme = ["https"]

# When links are available using HTTPS, treat HTTP links as errors.
require_https = false

# Request method
method = "get"

# Custom request headers
header = { "accept" = "text/html", "x-custom-header" = "value" }

# Remap URI matching pattern to different URI.
remap = ["https://example.com http://example.invalid"]

# Fallback extensions to apply when a URL does not specify one.
# This is common in documentation tools that cross-reference files without extensions.
fallback_extensions = ["md", "html"]

# Base URL or website root directory to check relative URLs.
base_url = "https://example.com"

# Root path to use when checking absolute local links, must be an absolute path
root_dir = "/dist"

# HTTP basic auth support. This will be the username and password passed to the
# authorization HTTP header. See
# <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Authorization>
basic_auth = ["example.com user:pwd"]

# Enable the checking of fragments in links.
include_fragments = true

# Minimum accepted TLS Version
min_tls = "TLSv1_3"

#############################  Exclusions  ##########################

# Skip missing input files (default is to error if they don't exist).
skip_missing = false

# Do not skip hidden directories and files
hidden = true

# Do not skip files that are ignored
no_ignore = true

# Check links inside `<code>` and `<pre>` blocks as well as Markdown code
# blocks.
include_verbatim = false

# Ignore case of paths when matching glob patterns.
glob_ignore_case = false

# Exclude URLs and mail addresses from checking. The values are treated as regular expressions
exclude = [
    '^https://example\.com/home$',                    # specific URL
    '^https?://example\.com',                         # match both HTTP and HTTPS
    '^https://(www\.)?linkedin\.com',                 # optional subdomains
    '^https://(.*\.)?github(usercontent)?\.(com|io)', # exclude common GitHub URLs
]

# Exclude paths from getting checked. The values are treated as regular expressions
exclude_path = [
    "\\.txt$",      # skip .txt extensions
    "(^|/)test/",   # skip directories named "test"
    "[aeiouAEIOU]", # exclude paths containing vowels
]

# Check the specified file extensions
extensions = ["md","txt","html"]

# URLs to check (supports regex). Has preference over all excludes.
include = ['gist\.github\.com.*']

# Exclude all private IPs from checking.
# Equivalent to setting `exclude_private`, `exclude_link_local`, and
# `exclude_loopback` to true.
exclude_all_private = false

# Exclude private IP address ranges from checking.
exclude_private = false

# Exclude link-local IP address range from checking.
exclude_link_local = false

# Exclude loopback IP address range and localhost from checking.
exclude_loopback = false

# Check mail addresses
include_mail = true

# Check WikiLinks in Markdown files
include_wikilinks = true

# Only check local files
offline = true

#############################  Other  #############################

# Read input filenames from the given file or stdin ('-')
files_from = "files.txt"

# Generate special output instead of performing link checking
generate = "man"

# Use the Wayback Machine as web archive
archive = "wayback"

# Search and suggest link replacements for all broken links
suggest = true
